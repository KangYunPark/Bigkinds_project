{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "ex_text = input('작성하고자 하는 뉴스의 카테고리를 입력해주세요 : ')\n",
        "print(\"\\n\" + '작성하시고자 하는 카테고리가 다음과 같습니까? ' + ex_text)\n",
        "print(\"\\n\" + '맞으면 숫자 1, 아니면 2를 눌러주세요')\n",
        "input_number = input()\n",
        "\n",
        "if input_number == 1:\n",
        "  ex_text = ex_text\n",
        "  print(ex_text + '로 선택되었습니다.')\n",
        "elif input_number == 2:\n",
        "  ex_text = input('작성하고자 하는 뉴스의 카테고리를 입력해주세요 : ')\n",
        "  ex_text = ex_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O1AIE24GQmSL",
        "outputId": "784b9b7b-d14d-4355-cad1-3248f95e754a"
      },
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "작성하고자 하는 뉴스의 카테고리를 입력해주세요 : 취업\n",
            "\n",
            "작성하시고자 하는 카테고리가 다음과 같습니까? 취업\n",
            "\n",
            "맞으면 숫자 1, 아니면 2를 눌러주세요\n",
            "1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ex_text"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "rux0kdOwTgwW",
        "outputId": "4bae0e0d-9394-4be8-ac49-ef0710f25bb9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'스포츠'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 54
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxEol5m8Y-p8"
      },
      "outputs": [],
      "source": [
        "from urllib import request, parse\n",
        "from pprint import pprint\n",
        "import json\n",
        "headers = {'Content-Type': 'application/json; chearset=utf-8'}\n",
        "data = {\n",
        "    'access_key': 'c623a12a-2a15-44bc-8d93-183b5b6834b4',\n",
        "    'argument': {\n",
        "        'query' : '취업',\n",
        "        'published_at': {\n",
        "            'from' : '2022-08-01',\n",
        "            'until' : '2022-08-31'\n",
        "        },\n",
        "        'return_from' : 0,\n",
        "        'return_size' : 1000,\n",
        "        \"fields\" : ['content']\n",
        "        }\n",
        "        }\n",
        "\n",
        "url = 'http://tools.kinds.or.kr:8888/search/news'\n",
        "req = request.Request(url, headers=headers, data=json.dumps(data).encode('utf-8'))\n",
        "res = request.urlopen(req)\n",
        "\n",
        "\n",
        "json_str = res.read().decode('utf-8')\n",
        "json_file = json.loads(json_str)\n",
        "json_file[:1]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the data\n",
        "data_list = []\n",
        "for i in range(0, 1000):\n",
        "  x = json_file['return_object']['documents'][i]['content']\n",
        "  data_list.append(x)"
      ],
      "metadata": {
        "id": "6qqtEcT_ZCb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "import re\n",
        "data_preprocessing = []\n",
        "for i in range(len(data_list)):\n",
        "  k = str(data_list[i].replace('\\n', '')) # replace 활용\n",
        "  k = re.sub('[a-zA-Z]', '', k) # re package 사용\n",
        "  k = re.sub('@', '', k)\n",
        "  k = re.sub(\"\\xa0\", '', k)\n",
        "  k = re.sub(\"’\", '', k)\n",
        "  k = re.sub('“', '', k)\n",
        "  k = re.sub(\"'\", '', k)\n",
        "  k = re.sub(\"‘\", '', k)\n",
        "  k = re.sub('”', '', k)\n",
        "  k = re.sub('[^\\w\\s]', '', k)\n",
        "  data_preprocessing.append(k)"
      ],
      "metadata": {
        "id": "tE9SHFJaZIX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kobert-transformers"
      ],
      "metadata": {
        "id": "MaJWgj9ZZM5D",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "42adc809-3b72-433c-bf51-ee15ba187bdc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kobert-transformers\n",
            "  Downloading kobert_transformers-0.5.1-py3-none-any.whl (12 kB)\n",
            "Requirement already satisfied: torch>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from kobert-transformers) (1.12.1+cu113)\n",
            "Collecting transformers<5,>=3\n",
            "  Downloading transformers-4.22.1-py3-none-any.whl (4.9 MB)\n",
            "\u001b[K     |████████████████████████████████| 4.9 MB 2.1 MB/s \n",
            "\u001b[?25hCollecting sentencepiece>=0.1.91\n",
            "  Downloading sentencepiece-0.1.97-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.3 MB 34.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.1.0->kobert-transformers) (4.1.1)\n",
            "Collecting tokenizers!=0.11.3,<0.13,>=0.11.1\n",
            "  Downloading tokenizers-0.12.1-cp37-cp37m-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (6.6 MB)\n",
            "\u001b[K     |████████████████████████████████| 6.6 MB 45.9 MB/s \n",
            "\u001b[?25hCollecting huggingface-hub<1.0,>=0.9.0\n",
            "  Downloading huggingface_hub-0.9.1-py3-none-any.whl (120 kB)\n",
            "\u001b[K     |████████████████████████████████| 120 kB 68.2 MB/s \n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (3.8.0)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (2022.6.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (2.23.0)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (6.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (21.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (4.12.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers<5,>=3->kobert-transformers) (1.21.6)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers<5,>=3->kobert-transformers) (3.0.9)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers<5,>=3->kobert-transformers) (3.8.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers<5,>=3->kobert-transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers, sentencepiece, kobert-transformers\n",
            "Successfully installed huggingface-hub-0.9.1 kobert-transformers-0.5.1 sentencepiece-0.1.97 tokenizers-0.12.1 transformers-4.22.1\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#data tokenization by kobert\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "data_kobert_token = []\n",
        "for i in range(0, 1000):\n",
        "  k = tokenizer.tokenize(\"[CLS]\" + data_preprocessing[i] + \"[SEP]\")\n",
        "  j = tokenizer.convert_tokens_to_ids(k) \n",
        "  data_kobert_token.append(j)\n",
        "\n",
        "#check vocab size\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "XXMG6vgjZPKB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0894eec3-7d32-4901-8bf4-7c8f977593bd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
            "The tokenizer class you load from this checkpoint is 'BertTokenizer'. \n",
            "The class this function is called from is 'KoBertTokenizer'.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "8002\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#check the length of sequences\n",
        "\n",
        "print('max_length : {}'.format(max(len(i) for i in data_kobert_token)))\n",
        "print('average : {}'.format(sum(map(len, data_kobert_token))/len(data_kobert_token)))"
      ],
      "metadata": {
        "id": "mxjuYVsFZjdD",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7fe60a18-2b57-4274-c905-5d2a2ff5eadd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "max_length : 1972\n",
            "average : 385.797\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#padding(pre)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense\n",
        "max_length = 512\n",
        "data_padding = pad_sequences(data_kobert_token, maxlen = max_length)"
      ],
      "metadata": {
        "id": "vG4l6mvpZq5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build Attention \n",
        "import tensorflow as tf\n",
        "\n",
        "class Att(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(Att, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def call(self, query, values):\n",
        "    # query hidden state -> (batch_size, hidden size)\n",
        "    # query_with_time_axis -> (batch_size, 1, hidden size)\n",
        "    # values -> (batch_size, max_len, hidden size)\n",
        "    # to get score, have to plus, so, expand time series vector\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score -> (batch_size, max_length, 1)\n",
        "    # score get only  one value sclar because of self.V layer\n",
        "    # self.V -> (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights -> (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    #context_vector -> (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "v-DSWV23ceeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset by tensorflow\n",
        "#setting the hyper prameters\n",
        "buffer_size = len(data_padding) # shuffle space \n",
        "batch_size= 1000\n",
        "# steps_per_epoch = len(data_padding)//batch_size\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "#tensor_slices -> traning, target but here, we don't have target data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data_padding).shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder -> last data drop or not?\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder_units = units\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.GRU = tf.keras.layers.GRU(self.encoder_units,\n",
        "                                     return_sequences = True,\n",
        "                                     return_state = True,\n",
        "                                     recurrent_initializer = 'glorot_uniform')\n",
        "\n",
        "  def call(self, data, hidden):\n",
        "    data =  self.embedding_layer(data)\n",
        "    output, state = self.GRU(data, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.encoder_units))"
      ],
      "metadata": {
        "id": "sjr9SunkviJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#next and iter function\n",
        "data_input = next(iter(dataset))\n",
        "print(data_input.shape)"
      ],
      "metadata": {
        "id": "YBgW0AIFzjs1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9a7eddbb-db92-449e-97f2-38653cd4ea98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(1000, 512)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(vocab_size, embedding_dim, units, batch_size)\n",
        "\n",
        "hidden_layer = encoder.hidden_state()\n",
        "layer_output, hidden_layer = encoder(data_input, hidden_layer)\n",
        "\n",
        "print(hidden_layer)"
      ],
      "metadata": {
        "id": "92rKkkqu0jc4",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b021af9-24ba-469f-ed31-7c59035fd6a0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[[-0.00243207  0.00319222  0.01027524 ... -0.00347601 -0.00044311\n",
            "   0.00791706]\n",
            " [-0.0014424  -0.0007148   0.01051171 ... -0.00060891 -0.00206686\n",
            "   0.01002387]\n",
            " [-0.0030315   0.00226851  0.01247618 ... -0.00054188 -0.00011752\n",
            "   0.01342034]\n",
            " ...\n",
            " [-0.01068841 -0.0019068   0.00577435 ...  0.00108308 -0.00619052\n",
            "   0.00943835]\n",
            " [-0.00539485 -0.0014702   0.01128644 ...  0.00462034  0.00330396\n",
            "   0.01416178]\n",
            " [-0.00117034  0.00232054  0.01129279 ... -0.0010222  -0.00092475\n",
            "   0.01183183]], shape=(1000, 1024), dtype=float32)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = Att(16)\n",
        "context_vector, weights = attention_layer(hidden_layer, layer_output)"
      ],
      "metadata": {
        "id": "yvKvwylL4L7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoder_units = decoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.decoder_units,\n",
        "                                   return_sequences = True,\n",
        "                                   return_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fullyconnect = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = Att(self.decoder_units)\n",
        "\n",
        "  def call(self, data, hidden, encoder_output):\n",
        "    context_vector, attention_weights = self.attention(hidden, encoder_output)\n",
        "    data = self.embedding(data)\n",
        "    data = tf.concat([tf.expand_dims(context_vector, 1), data], axis = -1)\n",
        "    output, state = self.gru(data)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    data = self.fullyconnect(output)\n",
        "\n",
        "    return data, state, attention_weights"
      ],
      "metadata": {
        "id": "Ibn8EewN4hgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, embedding_dim, units, batch_size)\n",
        "\n",
        "decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n",
        "                               hidden_layer, layer_output) "
      ],
      "metadata": {
        "id": "liHD6VfCOZlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder_output[1]"
      ],
      "metadata": {
        "id": "1E2TSxp3PILG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d132420-9f43-4a16-f80f-8262339a7cd0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tf.Tensor: shape=(8002,), dtype=float32, numpy=\n",
              "array([ 0.00045828,  0.002942  , -0.00103849, ..., -0.00237093,\n",
              "       -0.00137342,  0.00231997], dtype=float32)>"
            ]
          },
          "metadata": {},
          "execution_count": 42
        }
      ]
    }
  ]
}