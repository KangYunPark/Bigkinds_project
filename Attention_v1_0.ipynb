{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RxEol5m8Y-p8"
      },
      "outputs": [],
      "source": [
        "from urllib import request, parse\n",
        "from pprint import pprint\n",
        "import json\n",
        "headers = {'Content-Type': 'application/json; chearset=utf-8'}\n",
        "data = {\n",
        "    'access_key': 'c623a12a-2a15-44bc-8d93-183b5b6834b4',\n",
        "    'argument': {\n",
        "        'query' : '취업',\n",
        "        'published_at': {\n",
        "            'from' : '2022-08-01',\n",
        "            'until' : '2022-08-31'\n",
        "        },\n",
        "        'return_from' : 0,\n",
        "        'return_size' : 1000,\n",
        "        \"fields\" : ['content']\n",
        "        }\n",
        "        }\n",
        "\n",
        "url = 'http://tools.kinds.or.kr:8888/search/news'\n",
        "req = request.Request(url, headers=headers, data=json.dumps(data).encode('utf-8'))\n",
        "res = request.urlopen(req)\n",
        "\n",
        "\n",
        "json_str = res.read().decode('utf-8')\n",
        "json_file = json.loads(json_str)\n",
        "json_file"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#get the data\n",
        "data_list = []\n",
        "for i in range(0, 1000):\n",
        "  x = json_file['return_object']['documents'][i]['content']\n",
        "  data_list.append(x)"
      ],
      "metadata": {
        "id": "6qqtEcT_ZCb6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data preprocessing\n",
        "import re\n",
        "data_preprocessing = []\n",
        "for i in range(len(data_list)):\n",
        "  k = str(data_list[i].replace('\\n', '')) # replace 활용\n",
        "  k = re.sub('[a-zA-Z]', '', k) # re package 사용\n",
        "  k = re.sub('@', '', k)\n",
        "  k = re.sub(\"\\xa0\", '', k)\n",
        "  k = re.sub(\"’\", '', k)\n",
        "  k = re.sub('“', '', k)\n",
        "  k = re.sub(\"'\", '', k)\n",
        "  k = re.sub(\"‘\", '', k)\n",
        "  k = re.sub('”', '', k)\n",
        "  k = re.sub('[^\\w\\s]', '', k)\n",
        "  data_preprocessing.append(k)"
      ],
      "metadata": {
        "id": "tE9SHFJaZIX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pip install kobert-transformers"
      ],
      "metadata": {
        "id": "MaJWgj9ZZM5D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#data tokenization by kobert\n",
        "from kobert_transformers import get_tokenizer\n",
        "\n",
        "tokenizer = get_tokenizer()\n",
        "\n",
        "k  = tokenizer.tokenize(\"[CLS]\" + data_preprocessing[2] + \"[SEP]\")\n",
        "\n",
        "tokenizer.convert_tokens_to_ids(k)\n",
        "\n",
        "#check vocab size\n",
        "vocab_size = tokenizer.vocab_size\n",
        "print(tokenizer.vocab_size)"
      ],
      "metadata": {
        "id": "XXMG6vgjZPKB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#check the length of sequences\n",
        "\n",
        "print('max_length : {}'.format(max(len(i) for i in data_kobert_token)))\n",
        "print('average : {}'.format(sum(map(len, data_kobert_token))/len(data_kobert_token)))"
      ],
      "metadata": {
        "id": "mxjuYVsFZjdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#padding(pre)\n",
        "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
        "from tensorflow.keras.layers import Dense\n",
        "max_length = 512\n",
        "data_padding = pad_sequences(data_kobert_token, maxlen = max_length)"
      ],
      "metadata": {
        "id": "vG4l6mvpZq5V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#build Attention \n",
        "import tensorflow as tf\n",
        "\n",
        "class Att(tf.keras.Model):\n",
        "  def __init__(self, units):\n",
        "    super(Att, self).__init__()\n",
        "    self.W1 = Dense(units)\n",
        "    self.W2 = Dense(units)\n",
        "    self.V = Dense(1)\n",
        "\n",
        "  def get_score(self, query, values):\n",
        "    # query hidden state -> (batch_size, hidden size)\n",
        "    # query_with_time_axis -> (batch_size, 1, hidden size)\n",
        "    # values -> (batch_size, max_len, hidden size)\n",
        "    # to get score, have to plus, so, expand time series vector\n",
        "    query_with_time_axis = tf.expand_dims(query, 1)\n",
        "\n",
        "    # score -> (batch_size, max_length, 1)\n",
        "    # score get only  one value sclar because of self.V layer\n",
        "    # self.V -> (batch_size, max_length, units)\n",
        "    score = self.V(tf.nn.tanh(\n",
        "        self.W1(query_with_time_axis) + self.W2(values)))\n",
        "\n",
        "    # attention_weights -> (batch_size, max_length, 1)\n",
        "    attention_weights = tf.nn.softmax(score, axis=1)\n",
        "\n",
        "    #context_vector -> (batch_size, hidden_size)\n",
        "    context_vector = attention_weights * values\n",
        "    context_vector = tf.reduce_sum(context_vector, axis=1)\n",
        "\n",
        "    return context_vector, attention_weights\n",
        "\n"
      ],
      "metadata": {
        "id": "v-DSWV23ceeg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#prepare dataset by tensorflow\n",
        "#setting the hyper prameters\n",
        "buffer_size = len(data_padding) # shuffle space \n",
        "batch_size= 16\n",
        "# steps_per_epoch = len(data_padding)//batch_size\n",
        "embedding_dim = 256\n",
        "units = 1024\n",
        "\n",
        "#tensor_slices -> traning, target but here, we don't have target data\n",
        "dataset = tf.data.Dataset.from_tensor_slices(data_padding).shuffle(buffer_size)\n",
        "dataset = dataset.batch(batch_size, drop_remainder=True) # drop_remainder -> last data drop or not?\n",
        "\n",
        "class Encoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, units, batch_size):\n",
        "    super(Encoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.encoder_units = units\n",
        "    self.embedding_layer = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.GRU = tf.keras.layers.GRU(self.encoder_units,\n",
        "                                     return_sequences = True,\n",
        "                                     return_state = True,\n",
        "                                     recurrent_initializer = 'glorot_uniform')\n",
        "\n",
        "  def call(self, data, hidden):\n",
        "    data =  self.embedding_layer(data)\n",
        "    output, state = self.GRU(data, initial_state = hidden)\n",
        "    return output, state\n",
        "\n",
        "  def hidden_state(self):\n",
        "    return tf.zeros((self.batch_size, self.encoder_units))"
      ],
      "metadata": {
        "id": "sjr9SunkviJ6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#next and iter function\n",
        "data_input = next(iter(dataset))\n",
        "print(data_input.shape)"
      ],
      "metadata": {
        "id": "YBgW0AIFzjs1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "encoder = Encoder(data_input, vocab_size, embedding_dim, units, batch_size))\n",
        "\n",
        "hidden_layer = encdoer.hidden_state()\n",
        "layer_output, hidden_layer = encdoer(data_input, hidden_layer)\n",
        "\n",
        "print(hidden_layer)"
      ],
      "metadata": {
        "id": "92rKkkqu0jc4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "attention_layer = Att(16)\n",
        "context_vector, weights = attntion_layer(hidden_layer, layer_output)"
      ],
      "metadata": {
        "id": "yvKvwylL4L7P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Decoder\n",
        "\n",
        "class Decoder(tf.keras.Model):\n",
        "  def __init__(self, vocab_size, embedding_dim, decoder_units, batch_size):\n",
        "    super(Decoder, self).__init__()\n",
        "    self.batch_size = batch_size\n",
        "    self.decoder_units = decoder_units\n",
        "    self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
        "    self.gru = tf.keras.layers.GRU(self.decoder_units,\n",
        "                                   return_sequences = True,\n",
        "                                   reutrn_state = True,\n",
        "                                   recurrent_initializer = 'glorot_uniform')\n",
        "    self.fullyconnect = tf.keras.layers.Dense(vocab_size)\n",
        "\n",
        "    self.attention = Att(self.decoder_units)\n",
        "\n",
        "  def call(self, data, hidden, eoncoder_output):\n",
        "    context_vector, attention_seights = self.attention(hidden, encoder_output)\n",
        "    data = self.embedding(data)\n",
        "    data = tf.concat([tf.expand_dims(context_vector, 1), data], axis = -1)\n",
        "    oupput, state = self.gru(data)\n",
        "    output = tf.reshape(output, (-1, output.shape[2]))\n",
        "    data = self.fullyconnect(output)\n",
        "\n",
        "    return data, state, attention_weights"
      ],
      "metadata": {
        "id": "Ibn8EewN4hgk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoder = Decoder(vocab_size, embedding_dim, units, batch_size)\n",
        "\n",
        "decoder_output, _, _ = decoder(tf.random.uniform((batch_size, 1)),\n",
        "                               hidden_layer, layer_output)                               )"
      ],
      "metadata": {
        "id": "liHD6VfCOZlm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "decoer_output"
      ],
      "metadata": {
        "id": "1E2TSxp3PILG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}